#Model training dir
model_name: ./models--CYFRAGOVPL--Llama-PLLuM-8B-chat
train_data_path: ./train.json
output_lora_dir: ./pllum-lora-model

#Model training params
batch_size: 1
gradient_accumulation: 8
learning_rate: 2e-4
epochs: 3
logging_steps: 10
save_strategy: "no"
save_total_limit: 2
optimizer: paged_adamw_8bit

#Tokenizer
tokenizer_max_length: 512
tokenizer_padding: max_length
tokenizer_truncation: True

#Data collector
data_collector_padding: 8
data_collector_tensor_type: pt 

#Lora params
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_bias: "none"
lora_task_type: CAUSAL_LM

#Quantization params
load_in_4bit: True
use_double_quant: True
quant_type: nf4

